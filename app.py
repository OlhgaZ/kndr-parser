"""
ĞšĞĞ”Ğ -Ğ¿Ğ°Ñ€ÑĞµÑ€ â€” ĞšĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†
Ğ”ĞµĞ¿Ğ»Ğ¾Ğ¹: Streamlit Community Cloud
Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: ScrapingBee API (JS Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³, Ğ¾Ğ±Ñ…Ğ¾Ğ´ Cloudflare)
"""

import io
import re
import json
import string
import asyncio
import concurrent.futures
from urllib.parse import urlparse
from collections import defaultdict

import httpx
import streamlit as st
import pandas as pd
from bs4 import BeautifulSoup
import openpyxl
from openpyxl.styles import PatternFill, Font, Alignment, Border, Side
from openpyxl.utils import get_column_letter

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¡Ğ›ĞĞ’ĞĞ Ğ¬ Ğ¡Ğ˜ĞĞĞĞ˜ĞœĞĞ’ (Ñ€ÑƒÑÑĞºĞ¸Ğ¹ + Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SYNONYM_GROUPS = {
    "ğŸ¦¸ Ğ“ĞµÑ€Ğ¾Ğ¹ / Hero": [
        # RU
        "Ğ³ĞµÑ€Ğ¾Ğ¹", "Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹", "Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞºÑ€Ğ°Ğ½", "Ğ¾Ğ±Ğ»Ğ¾Ğ¶ĞºĞ°", "Ğ±Ğ°Ğ½Ğ½ĞµÑ€", "ÑĞ»Ğ°Ğ¹Ğ´ĞµÑ€", "ÑĞ»Ğ°Ğ¹Ğ´",
        "Ğ´Ğ¾Ğ±Ñ€Ğ¾ Ğ¿Ğ¾Ğ¶Ğ°Ğ»Ğ¾Ğ²Ğ°Ñ‚ÑŒ", "Ğ¼Ñ‹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµĞ¼", "Ğ¼Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼", "Ğ¼Ñ‹ Ğ´ĞµĞ»Ğ°ĞµĞ¼",
        # EN
        "hero", "welcome", "banner", "slider", "above the fold", "headline",
        "we help", "we create", "we build", "get started",
    ],
    "â­ ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°": [
        "Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾", "Ğ²Ñ‹Ğ³Ğ¾Ğ´Ğ°", "Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ°", "Ğ¿Ğ»ÑÑ", "Ğ´Ğ¾ÑÑ‚Ğ¾Ğ¸Ğ½ÑÑ‚Ğ²Ğ¾", "Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ",
        "Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ¼Ñ‹", "Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ½Ğ°Ñ", "Ğ·Ğ°Ñ‡ĞµĞ¼", "Ñ‡ĞµĞ¼ Ğ¼Ñ‹ Ğ»ÑƒÑ‡ÑˆĞµ", "Ğ½Ğ°ÑˆĞ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°",
        "why us", "why choose", "benefits", "advantages", "features", "what makes",
    ],
    "ğŸ› ï¸ Ğ£ÑĞ»ÑƒĞ³Ğ¸ / ĞŸÑ€Ğ¾Ğ´ÑƒĞºÑ‚Ñ‹": [
        "ÑƒÑĞ»ÑƒĞ³Ğ°", "ÑĞµÑ€Ğ²Ğ¸Ñ", "Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ", "Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚", "Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ", "Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ",
        "Ñ‡Ñ‚Ğ¾ Ğ¼Ñ‹ Ğ´ĞµĞ»Ğ°ĞµĞ¼", "Ñ‡Ñ‚Ğ¾ Ğ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµĞ¼", "Ğ½Ğ°ÑˆĞ¸ ÑƒÑĞ»ÑƒĞ³Ğ¸", "Ğ½Ğ°ÑˆĞ¸ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ñ‹",
        "services", "products", "solutions", "offerings", "what we do",
    ],
    "ğŸ’° Ğ¦ĞµĞ½Ñ‹ / Ğ¢Ğ°Ñ€Ğ¸Ñ„Ñ‹": [
        "Ñ†ĞµĞ½Ğ°", "ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ", "Ñ‚Ğ°Ñ€Ğ¸Ñ„", "Ğ¿Ñ€Ğ°Ğ¹Ñ", "Ñ€Ğ°ÑÑ†ĞµĞ½ĞºĞ°", "ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ¾Ğ¸Ñ‚",
        "Ğ¿Ğ°ĞºĞµÑ‚", "Ñ‚Ğ°Ñ€Ğ¸Ñ„Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½", "Ğ¾Ğ¿Ğ»Ğ°Ñ‚Ğ°", "ÑÑ‚Ğ¾Ğ¸Ñ‚ÑŒ",
        "price", "pricing", "plans", "packages", "cost", "rates", "tariff",
    ],
    "ğŸ’¬ ĞÑ‚Ğ·Ñ‹Ğ²Ñ‹": [
        "Ğ¾Ñ‚Ğ·Ñ‹Ğ²", "Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ", "Ğ¾Ñ†ĞµĞ½ĞºĞ°", "Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ", "Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ñ", "Ñ‡Ñ‚Ğ¾ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‚",
        "Ğ½Ğ°ÑˆĞ¸ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹", "Ğ½Ğ°Ğ¼ Ğ´Ğ¾Ğ²ĞµÑ€ÑÑÑ‚",
        "testimonials", "reviews", "feedback", "what our clients say", "what people say",
    ],
    "â“ FAQ": [
        "faq", "Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ", "Ğ¾Ñ‚Ğ²ĞµÑ‚", "Ñ‡Ğ°ÑÑ‚Ñ‹Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ", "Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°ÑÑ‚",
        "frequently asked", "questions", "q&a", "answers",
    ],
    "ğŸ‘¥ ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°": [
        "ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°", "ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ğº", "ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚", "ÑĞºÑĞ¿ĞµÑ€Ñ‚", "Ğ¼Ğ°ÑÑ‚ĞµÑ€", "Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»",
        "Ğ½Ğ°ÑˆĞ° ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°", "Ğ¿Ğ¾Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼ÑŒÑ‚ĞµÑÑŒ",
        "team", "our team", "meet", "staff", "experts", "specialists",
    ],
    "ğŸ¢ Ğ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸": [
        "ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ", "Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ", "ÑÑ‚ÑƒĞ´Ğ¸Ñ", "Ğ°Ğ³ĞµĞ½Ñ‚ÑÑ‚Ğ²Ğ¾", "ĞºÑ‚Ğ¾ Ğ¼Ñ‹", "Ğ¾ Ğ½Ğ°Ñ",
        "Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ", "Ğ¼Ğ¸ÑÑĞ¸Ñ", "Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸", "Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸",
        "about", "about us", "our story", "mission", "vision", "who we are",
    ],
    "ğŸ“ ĞšĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹": [
        "ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚", "ÑĞ²ÑĞ·ÑŒ", "Ğ°Ğ´Ñ€ĞµÑ", "Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½", "Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ", "Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ½Ğ¸Ñ‚ÑŒ",
        "Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ", "ÑĞ²ÑĞ¶Ğ¸Ñ‚ĞµÑÑŒ", "Ñ„Ğ¾Ñ€Ğ¼Ğ° ÑĞ²ÑĞ·Ğ¸",
        "contact", "contacts", "get in touch", "reach us", "write to us",
    ],
    "ğŸ—‚ï¸ ĞŸĞ¾Ñ€Ñ‚Ñ„Ğ¾Ğ»Ğ¸Ğ¾ / ĞšĞµĞ¹ÑÑ‹": [
        "Ğ¿Ğ¾Ñ€Ñ‚Ñ„Ğ¾Ğ»Ğ¸Ğ¾", "ĞºĞµĞ¹Ñ", "Ğ¿Ñ€Ğ¾ĞµĞºÑ‚", "Ğ½Ğ°ÑˆĞ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹", "Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ñ€Ğ°Ğ±Ğ¾Ñ‚",
        "portfolio", "case studies", "our work", "projects", "examples",
    ],
    "ğŸ¤ ĞŸĞ°Ñ€Ñ‚Ğ½Ñ‘Ñ€Ñ‹ / ĞšĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹": [
        "Ğ¿Ğ°Ñ€Ñ‚Ğ½Ñ‘Ñ€", "Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€", "ĞºĞ»Ğ¸ĞµĞ½Ñ‚", "Ğ»Ğ¾Ğ³Ğ¾Ñ‚Ğ¸Ğ¿", "Ğ±Ñ€ĞµĞ½Ğ´", "Ğ½Ğ°Ğ¼ Ğ´Ğ¾Ğ²ĞµÑ€ÑÑÑ‚",
        "Ñ Ğ½Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚", "Ğ½Ğ°ÑˆĞ¸ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹",
        "partners", "clients", "trusted by", "brands", "our clients",
    ],
    "ğŸ›¡ï¸ Ğ“Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¸": [
        "Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ", "Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ", "Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾", "ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ", "Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ",
        "guarantee", "guarantees", "warranty", "commitment", "reliability",
    ],
    "ğŸ“‹ ĞŸÑ€Ğ¾Ñ†ĞµÑÑ / Ğ­Ñ‚Ğ°Ğ¿Ñ‹": [
        "Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ", "ÑÑ‚Ğ°Ğ¿", "ÑˆĞ°Ğ³", "ĞºĞ°Ğº Ğ¼Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµĞ¼", "Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹",
        "ÑÑ…ĞµĞ¼Ğ°", "ĞºĞ°Ğº ÑÑ‚Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚",
        "process", "how it works", "steps", "our process", "workflow",
    ],
    "ğŸ“Š Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° / Ğ¦Ğ¸Ñ„Ñ€Ñ‹": [
        "ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°", "Ñ†Ğ¸Ñ„Ñ€Ğ°", "Ñ„Ğ°ĞºÑ‚", "Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ", "Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚", "Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ",
        "Ğ½Ğ°Ñ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ»Ğ¸", "Ğ´Ğ¾Ğ²ĞµÑ€ÑÑÑ‚",
        "stats", "statistics", "numbers", "achievements", "results", "facts",
    ],
    "ğŸ“ Ğ‘Ğ»Ğ¾Ğ³ / Ğ¡Ñ‚Ğ°Ñ‚ÑŒĞ¸": [
        "Ğ±Ğ»Ğ¾Ğ³", "ÑÑ‚Ğ°Ñ‚ÑŒÑ", "Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ÑŒ", "Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ", "Ğ¿Ğ¾ÑÑ‚", "Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»",
        "blog", "articles", "news", "posts", "latest", "insights",
    ],
    "ğŸ¥ Ğ’Ğ¸Ğ´ĞµĞ¾": [
        "Ğ²Ğ¸Ğ´ĞµĞ¾", "Ñ€Ğ¾Ğ»Ğ¸Ğº", "Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ", "Ğ´ĞµĞ¼Ğ¾", "ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ",
        "video", "watch", "demo", "presentation",
    ],
    "ğŸ“© CTA / Ğ—Ğ°ÑĞ²ĞºĞ°": [
        "Ğ·Ğ°ÑĞ²ĞºĞ°", "Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒÑÑ", "Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ", "Ğ·Ğ°ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ", "Ğ¿Ğ¾Ğ¿Ñ€Ğ¾Ğ±Ğ¾Ğ²Ğ°Ñ‚ÑŒ",
        "Ğ½Ğ°Ñ‡Ğ°Ñ‚ÑŒ", "ĞºÑƒĞ¿Ğ¸Ñ‚ÑŒ", "Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ·Ğ°ÑĞ²ĞºÑƒ", "Ğ±ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ğ¾",
        "cta", "call to action", "sign up", "get started", "try free",
        "book", "order", "buy", "request",
    ],
}


def build_keyword_index() -> dict[str, str]:
    """Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ keyword â†’ group name"""
    idx = {}
    for group, keywords in SYNONYM_GROUPS.items():
        for kw in keywords:
            idx[kw.lower()] = group
    return idx


KEYWORD_INDEX = build_keyword_index()


def normalize(text: str) -> str:
    """ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: lower + ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿ÑƒĞ½ĞºÑ‚ÑƒĞ°Ñ†Ğ¸Ğ¸"""
    text = text.lower()
    text = re.sub(r"[^\w\s]", " ", text, flags=re.UNICODE)
    return re.sub(r"\s+", " ", text).strip()


def assign_group(heading: str) -> str:
    """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñƒ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼"""
    norm = normalize(heading)

    # Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸Ñ‰ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ„Ñ€Ğ°Ğ·Ñ‹ (Ğ´Ğ»Ğ¸Ğ½Ğ½ĞµĞµ â€” Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ)
    sorted_kws = sorted(KEYWORD_INDEX.keys(), key=len, reverse=True)
    for kw in sorted_kws:
        if kw in norm:
            return KEYWORD_INDEX[kw]

    # Fallback: Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ°
    words = [w for w in norm.split() if len(w) > 3]
    return " ".join(words[:3]) if words else heading[:25]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ Ğ¡Ğ¢Ğ ĞĞĞ˜Ğ¦ (ScrapingBee)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def fetch_via_scrapingbee(url: str, api_key: str, timeout: int = 30) -> tuple[str | None, str | None]:
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ Ñ‡ĞµÑ€ĞµĞ· ScrapingBee API Ñ JS-Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ¾Ğ¼"""
    endpoint = "https://app.scrapingbee.com/api/v1/"
    params = {
        "api_key": api_key,
        "url": url,
        "render_js": "true",
        "wait": "2000",           # Ğ¶Ğ´Ñ‘Ğ¼ 2 ÑĞµĞº Ğ¿Ğ¾ÑĞ»Ğµ JS
        "wait_for": "body",
        "scroll_to_bottom": "true",
        "premium_proxy": "false",
        "block_ads": "true",
        "block_resources": "false",
        "return_page_source": "true",
    }
    try:
        with httpx.Client(timeout=timeout + 10) as client:
            r = client.get(endpoint, params=params)
        if r.status_code == 200:
            return r.text, None
        elif r.status_code == 401:
            return None, "âŒ ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ API-ĞºĞ»ÑÑ‡ ScrapingBee"
        elif r.status_code == 422:
            return None, f"âŒ Ğ¡Ğ°Ğ¹Ñ‚ Ğ·Ğ°Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ (ĞºĞ¾Ğ´ 422)"
        elif r.status_code == 500:
            return None, "âŒ ScrapingBee: Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° ÑĞµÑ€Ğ²ĞµÑ€Ğ°"
        else:
            return None, f"âŒ HTTP {r.status_code}: {r.text[:200]}"
    except httpx.TimeoutException:
        return None, f"âŒ Ğ¢Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ ({timeout} ÑĞµĞº)"
    except Exception as e:
        return None, f"âŒ {str(e)[:150]}"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ĞŸĞĞ Ğ¡Ğ˜ĞĞ“ HTML â†’ Ğ‘Ğ›ĞĞšĞ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def extract_blocks(html: str, mode: str) -> list[dict]:
    """
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ñ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸.
    mode: 'main' | 'inner'
    """
    soup = BeautifulSoup(html, "lxml")

    # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¼ÑƒÑĞ¾Ñ€
    for tag in soup(["script", "style", "noscript", "svg", "meta", "link"]):
        tag.decompose()

    body = soup.find("body") or soup

    if mode == "inner":
        for rem in body.find_all(["header", "footer", "nav"]):
            rem.decompose()

    search_root = body
    heading_tags = ["h1", "h2", "h3", "h4", "h5", "h6"]
    blocks = []
    seen_ids = set()

    for heading in search_root.find_all(heading_tags):
        heading_text = heading.get_text(strip=True)
        if not heading_text or len(heading_text) < 2:
            continue

        # Ğ‘Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ğ¹ Ñ€Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ section/div/article
        block_el = None
        current = heading.parent
        for _ in range(8):
            if current is None or current.name in ("body", "html", "[document]"):
                block_el = heading.parent
                break
            if current.name in ("section", "div", "article", "main", "aside"):
                block_el = current
                break
            current = current.parent
        if block_el is None:
            block_el = heading.parent

        el_id = id(block_el)
        if el_id in seen_ids:
            continue
        seen_ids.add(el_id)

        text = block_el.get_text(separator=" ", strip=True)

        # ĞšĞ½Ğ¾Ğ¿ĞºĞ¸: <button> + <a> Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼
        buttons = block_el.find_all("button")
        links_cta = [
            a for a in block_el.find_all("a")
            if a.get_text(strip=True) and len(a.get_text(strip=True)) < 60
        ]
        forms = block_el.find_all("form")
        lists = block_el.find_all(["ul", "ol"])
        iframes = block_el.find_all("iframe")
        videos = block_el.find_all("video")
        tables = block_el.find_all("table")
        images = block_el.find_all("img")

        # FAQ Schema.org
        faq_schema = False
        for script in block_el.find_all("script", type="application/ld+json"):
            try:
                data = json.loads(script.string or "")
                items = [data] if isinstance(data, dict) else (data if isinstance(data, list) else [])
                for item in items:
                    if isinstance(item, dict) and item.get("@type") == "FAQPage":
                        faq_schema = True
            except Exception:
                pass

        blocks.append({
            "heading": heading_text,
            "level": int(heading.name[1]),
            "text_len": len(text),
            "buttons": len(buttons) + len(links_cta),
            "has_form": bool(forms),
            "has_list": bool(lists),
            "has_iframe": bool(iframes),
            "has_video": bool(videos),
            "has_table": bool(tables),
            "images": len(images),
            "has_faq_schema": faq_schema,
            "group": assign_group(heading_text),
        })

    return blocks


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  EXCEL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def make_excel(
    target_url: str,
    competitor_urls: list[str],
    all_results: dict[str, list[dict]],
) -> bytes:

    # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ğ±Ğ»Ğ¾ĞºĞ¸ Ğ¿Ğ¾ group
    all_groups: dict[str, dict[str, list[dict]]] = {}
    for url, blocks in all_results.items():
        all_groups[url] = defaultdict(list)
        for b in blocks:
            all_groups[url][b["group"]].append(b)

    # Ğ’ÑĞµ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹, Ğ¾Ñ‚ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ
    unique_groups: list[str] = []
    seen = set()
    # Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ² Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ Ğ¸Ğ· ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ (Ñ‚Ğµ Ñ‡Ñ‚Ğ¾ Ğ½Ğ°ÑˆĞ»Ğ¸)
    for g in SYNONYM_GROUPS.keys():
        for url in all_results:
            if g in all_groups.get(url, {}):
                if g not in seen:
                    unique_groups.append(g)
                    seen.add(g)
    # Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹
    for url in all_results:
        for g in all_groups.get(url, {}):
            if g not in seen:
                unique_groups.append(g)
                seen.add(g)

    all_urls = competitor_urls + [target_url]
    short = {u: urlparse(u).netloc or u for u in all_urls}

    wb = openpyxl.Workbook()

    # â”€â”€â”€ Ğ¡Ñ‚Ğ¸Ğ»Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    H_FILL   = PatternFill("solid", fgColor="1F3864")   # Ñ‚Ñ‘Ğ¼Ğ½Ğ¾-ÑĞ¸Ğ½Ğ¸Ğ¹ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº
    TGT_FILL = PatternFill("solid", fgColor="D9EAD3")   # Ğ·ĞµĞ»Ñ‘Ğ½Ñ‹Ğ¹ â€” Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞµÑÑ‚ÑŒ
    MISS_FILL= PatternFill("solid", fgColor="FCE4D6")   # ĞºÑ€Ğ°ÑĞ½Ñ‹Ğ¹ â€” Ğ±Ğ»Ğ¾Ğº Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚
    OBL_FILL = PatternFill("solid", fgColor="C6EFCE")   # Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾
    WISH_FILL= PatternFill("solid", fgColor="FFEB9C")   # Ğ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾
    OPT_FILL = PatternFill("solid", fgColor="F2F2F2")   # Ğ¿Ğ¾ Ğ¶ĞµĞ»Ğ°Ğ½Ğ¸Ñ
    ALT_FILL = PatternFill("solid", fgColor="EBF3FB")   # Ñ‡ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ¾Ğº (ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ñ‹)
    WHITE    = PatternFill("solid", fgColor="FFFFFF")

    H_FONT   = Font(bold=True, color="FFFFFF", size=10)
    BOLD     = Font(bold=True, size=10)
    NORM     = Font(size=10)
    CENTER   = Alignment(horizontal="center", vertical="center", wrap_text=True)
    LEFT     = Alignment(horizontal="left",   vertical="center", wrap_text=True)
    RIGHT    = Alignment(horizontal="right",  vertical="center")

    def border():
        s = Side(style="thin", color="D0D0D0")
        return Border(left=s, right=s, top=s, bottom=s)

    def set_header_row(ws, headers: list, row=1):
        for ci, h in enumerate(headers, 1):
            c = ws.cell(row=row, column=ci, value=h)
            c.fill = H_FILL; c.font = H_FONT
            c.alignment = CENTER; c.border = border()

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  Ğ›Ğ˜Ğ¡Ğ¢ 1 â€” Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ²
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    ws1 = wb.active
    ws1.title = "Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ²"

    comp_names = [short[u] for u in competitor_urls]
    cols = ["Ğ‘Ğ»Ğ¾Ğº"] + comp_names + [f"â˜… {short[target_url]}"] + ["Ğ§Ğ°ÑÑ‚Ğ¾Ñ‚Ğ°", "Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ñ"]
    set_header_row(ws1, cols)

    for ri, group in enumerate(unique_groups, 2):
        freq = 0
        row_vals = [group]

        for cu in competitor_urls:
            gdata = all_groups.get(cu, {}).get(group)
            if gdata:
                freq += 1
                row_vals.append(f"âœ“  {gdata[0]['heading'][:45]}")
            else:
                row_vals.append("â€”")

        tgt_data = all_groups.get(target_url, {}).get(group)
        target_has = bool(tgt_data)
        if target_has:
            row_vals.append(f"âœ“  {tgt_data[0]['heading'][:45]}")
        else:
            row_vals.append("ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ âœ—")

        row_vals.append(freq)

        if freq >= 3:
            rec, rec_fill = "ğŸ”´ ĞĞ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾", OBL_FILL
        elif freq == 2:
            rec, rec_fill = "ğŸŸ¡ Ğ–ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾", WISH_FILL
        else:
            rec, rec_fill = "âšª ĞŸĞ¾ Ğ¶ĞµĞ»Ğ°Ğ½Ğ¸Ñ", OPT_FILL
        row_vals.append(rec)

        row_bg = ALT_FILL if ri % 2 == 0 else WHITE

        for ci, val in enumerate(row_vals, 1):
            c = ws1.cell(row=ri, column=ci, value=val)
            c.border = border()
            c.alignment = LEFT if ci <= len(cols) - 2 else CENTER
            c.font = NORM

            if ci == 1:  # Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹
                c.font = BOLD
                c.fill = row_bg
            elif ci == len(cols) - 1:  # Ğ§Ğ°ÑÑ‚Ğ¾Ñ‚Ğ°
                c.alignment = CENTER
                c.fill = row_bg
            elif ci == len(cols):  # Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ñ
                c.fill = rec_fill
                c.font = BOLD
                c.alignment = CENTER
            elif ci == len(cols) - 2:  # ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾
                c.fill = MISS_FILL if not target_has else TGT_FILL
            else:
                c.fill = row_bg

    # Ğ¨Ğ¸Ñ€Ğ¸Ğ½Ğ°
    col_widths = [28] + [24] * len(competitor_urls) + [26, 10, 18]
    for i, w in enumerate(col_widths, 1):
        ws1.column_dimensions[get_column_letter(i)].width = w
    ws1.row_dimensions[1].height = 32
    ws1.freeze_panes = "B2"
    ws1.sheet_view.showGridLines = True

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  Ğ›Ğ˜Ğ¡Ğ¢ 2 â€” Ğ’ÑĞµ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    ws2 = wb.create_sheet("Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸ H1â€“H6")
    h2_cols = ["Ğ¡Ğ°Ğ¹Ñ‚", "Ğ£Ñ€Ğ¾Ğ²ĞµĞ½ÑŒ", "Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº", "Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ°", "Ğ”Ğ»Ğ¸Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°",
               "CTA-ĞºĞ½Ğ¾Ğ¿ĞºĞ¸", "Ğ¤Ğ¾Ñ€Ğ¼Ğ°", "Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº", "Ğ˜Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ", "FAQ-ÑÑ…ĞµĞ¼Ğ°"]
    set_header_row(ws2, h2_cols)

    ri2 = 2
    for url in all_urls:
        is_tgt = url == target_url
        for b in all_results.get(url, []):
            row = [
                short[url],
                f"H{b['level']}",
                b["heading"],
                b["group"],
                b["text_len"],
                b["buttons"],
                "Ğ”Ğ°" if b["has_form"] else "ĞĞµÑ‚",
                "Ğ”Ğ°" if b["has_list"] else "ĞĞµÑ‚",
                b["images"],
                "Ğ”Ğ°" if b["has_faq_schema"] else "ĞĞµÑ‚",
            ]
            row_bg = PatternFill("solid", fgColor="EBF3FB") if is_tgt else (ALT_FILL if ri2 % 2 == 0 else WHITE)
            for ci, val in enumerate(row, 1):
                c = ws2.cell(row=ri2, column=ci, value=val)
                c.border = border()
                c.font = BOLD if is_tgt else NORM
                c.fill = row_bg
                c.alignment = CENTER if ci in (2, 5, 6, 8, 9, 10) else LEFT
            ri2 += 1

    for i, w in enumerate([28, 9, 48, 26, 14, 12, 8, 8, 13, 12], 1):
        ws2.column_dimensions[get_column_letter(i)].width = w
    ws2.row_dimensions[1].height = 32
    ws2.freeze_panes = "A2"

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  Ğ›Ğ˜Ğ¡Ğ¢ 3 â€” Ğ¡Ğ²Ğ¾Ğ´Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    ws3 = wb.create_sheet("Ğ¡Ğ²Ğ¾Ğ´Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°")
    s_cols = ["Ğ¡Ğ°Ğ¹Ñ‚", "Ğ Ğ¾Ğ»ÑŒ", "Ğ‘Ğ»Ğ¾ĞºĞ¾Ğ² Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾", "CTA-ĞºĞ½Ğ¾Ğ¿Ğ¾Ğº", "Ğ¤Ğ¾Ñ€Ğ¼",
              "Ğ¡Ğ¿Ğ¸ÑĞºĞ¾Ğ²", "Ğ˜Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹", "ĞĞ±ÑŠÑ‘Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ° (ÑĞ¸Ğ¼Ğ².)", "FAQ-ÑÑ…ĞµĞ¼"]
    set_header_row(ws3, s_cols)

    for ri3, url in enumerate(all_urls, 2):
        blocks = all_results.get(url, [])
        is_tgt = url == target_url
        role = "â˜… ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹" if is_tgt else "ĞšĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚"
        row = [
            short[url], role,
            len(blocks),
            sum(b["buttons"]   for b in blocks),
            sum(1 for b in blocks if b["has_form"]),
            sum(1 for b in blocks if b["has_list"]),
            sum(b["images"]    for b in blocks),
            sum(b["text_len"]  for b in blocks),
            sum(1 for b in blocks if b["has_faq_schema"]),
        ]
        fill = PatternFill("solid", fgColor="D9EAD3") if is_tgt else (ALT_FILL if ri3 % 2 == 0 else WHITE)
        for ci, val in enumerate(row, 1):
            c = ws3.cell(row=ri3, column=ci, value=val)
            c.border = border()
            c.font = BOLD if is_tgt else NORM
            c.fill = fill
            c.alignment = CENTER if ci > 2 else LEFT

    for i, w in enumerate([30, 18, 16, 14, 10, 10, 14, 22, 13], 1):
        ws3.column_dimensions[get_column_letter(i)].width = w
    ws3.row_dimensions[1].height = 32
    ws3.freeze_panes = "A2"

    buf = io.BytesIO()
    wb.save(buf)
    buf.seek(0)
    return buf.read()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  STREAMLIT UI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def main():
    st.set_page_config(
        page_title="ĞšĞĞ”Ğ -Ğ¿Ğ°Ñ€ÑĞµÑ€",
        page_icon="ğŸ”",
        layout="wide",
        initial_sidebar_state="expanded",
    )

    # â”€â”€ CSS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("""
    <style>
    .main-title {
        font-size: 2.2rem; font-weight: 900;
        color: #1F3864; margin-bottom: 0; letter-spacing: -1px;
    }
    .subtitle {
        color: #555; margin-top: 2px; margin-bottom: 1.5rem; font-size: 1rem;
    }
    .metric-card {
        background: #f0f4f8; border-radius: 10px;
        padding: 12px 16px; text-align: center;
        border: 1px solid #d0dce8;
    }
    .status-ok  { color: #2e7d32; font-weight: 600; }
    .status-err { color: #c62828; font-weight: 600; }
    div[data-testid="stButton"] button[kind="primary"] {
        background: #1F3864; border: none;
    }
    </style>
    """, unsafe_allow_html=True)

    st.markdown('<p class="main-title">ğŸ” ĞšĞĞ”Ğ -Ğ¿Ğ°Ñ€ÑĞµÑ€</p>', unsafe_allow_html=True)
    st.markdown('<p class="subtitle">ĞšĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Â· JS-Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Â· Excel-Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚</p>',
                unsafe_allow_html=True)

    # â”€â”€ Ğ‘Ğ¾ĞºĞ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ğ½ĞµĞ»ÑŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.sidebar:
        st.header("âš™ï¸ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸")

        api_key = st.text_input(
            "ğŸ”‘ ScrapingBee API Key",
            type="password",
            help="ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚Ğµ Ğ±ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ñ‹Ğ¹ ĞºĞ»ÑÑ‡ Ğ½Ğ° scrapingbee.com (1000 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ±ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ğ¾)",
            placeholder="Ğ’ÑÑ‚Ğ°Ğ²ÑŒÑ‚Ğµ API-ĞºĞ»ÑÑ‡...",
        )
        if not api_key:
            st.info("ğŸ‘† Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ API-ĞºĞ»ÑÑ‡ ScrapingBee Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†.\n\n"
                    "[â†’ ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ±ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ñ‹Ğ¹ ĞºĞ»ÑÑ‡](https://www.scrapingbee.com/)")

        st.divider()

        mode = st.radio(
            "Ğ ĞµĞ¶Ğ¸Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°",
            ["ğŸ  Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°", "ğŸ“„ Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°"],
            help="Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ: Ğ²ÑÑ‘ Ñ‚ĞµĞ»Ğ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹.\nĞ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ: Ğ±ĞµĞ· header Ğ¸ footer.",
        )
        mode_key = "main" if "Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ" in mode else "inner"

        timeout = st.slider("Ğ¢Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ (ÑĞµĞº)", 15, 60, 30, step=5)

        st.divider()
        st.markdown("**ĞšĞ°Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚:**")
        st.markdown("""
1. ScrapingBee Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ñ JS-Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¾Ğ¼
2. BeautifulSoup Ğ¸Ñ‰ĞµÑ‚ Ğ±Ğ»Ğ¾ĞºĞ¸ Ñ H1â€“H6
3. Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ ÑĞ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ‘Ğ¼ (~70 Ğ³Ñ€ÑƒĞ¿Ğ¿ RU+EN)
4. Excel: ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Â· Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸ Â· ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
""")
        with st.expander("ğŸ“– Ğ“Ñ€ÑƒĞ¿Ğ¿Ñ‹ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ²"):
            for group in SYNONYM_GROUPS:
                st.markdown(f"- {group}")

    # â”€â”€ Ğ’Ğ²Ğ¾Ğ´ URL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    col1, col2 = st.columns([1, 1], gap="large")

    with col1:
        st.subheader("ğŸ¯ ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ÑĞ°Ğ¹Ñ‚")
        target_url = st.text_input(
            "URL ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹",
            placeholder="https://mysite.ru",
            key="target_url",
        )

    with col2:
        st.subheader("ğŸ† ĞšĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ñ‹")
        st.caption("ĞÑ‚ 4 Ğ´Ğ¾ 10 URL, ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾ĞºĞ¸")
        competitors_raw = st.text_area(
            "URL ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ²",
            placeholder=(
                "https://competitor1.ru\n"
                "https://competitor2.ru\n"
                "https://competitor3.ru\n"
                "https://competitor4.ru"
            ),
            height=170,
            key="competitors",
        )

    # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ²
    competitor_urls = [
        u.strip() for u in competitors_raw.strip().splitlines()
        if u.strip() and u.strip().startswith("http")
    ]

    # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ
    errors = []
    if target_url and not target_url.startswith("http"):
        errors.append("URL Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¹Ñ‚Ğ° Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ‚ÑŒÑÑ Ñ http:// Ğ¸Ğ»Ğ¸ https://")
    if competitor_urls and len(competitor_urls) < 4:
        errors.append(f"ĞÑƒĞ¶Ğ½Ğ¾ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ 4 ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ° â€” ÑĞµĞ¹Ñ‡Ğ°Ñ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¾: {len(competitor_urls)}")
    if competitor_urls and len(competitor_urls) > 10:
        errors.append(f"ĞœĞ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 10 ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² â€” ÑĞµĞ¹Ñ‡Ğ°Ñ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¾: {len(competitor_urls)}")

    for e in errors:
        st.warning(f"âš ï¸ {e}")

    can_run = bool(api_key) and bool(target_url) and bool(competitor_urls) and not errors

    # â”€â”€ ĞšĞ½Ğ¾Ğ¿ĞºĞ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.divider()
    col_run, col_dl, col_info = st.columns([2, 2, 4])

    with col_run:
        run_btn = st.button(
            "ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·",
            disabled=not can_run,
            use_container_width=True,
            type="primary",
        )

    if not api_key:
        with col_info:
            st.info("ğŸ”‘ Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ API-ĞºĞ»ÑÑ‡ ScrapingBee Ğ² Ğ±Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ°Ğ½ĞµĞ»Ğ¸")
    elif not target_url or not competitor_urls:
        with col_info:
            st.info("â¬†ï¸ Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚Ğµ URL Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¹Ñ‚Ğ° Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ²")

    # â”€â”€ ĞĞ½Ğ°Ğ»Ğ¸Ğ· â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if run_btn:
        all_urls = competitor_urls + [target_url]
        all_results: dict[str, list[dict]] = {}
        errors_log: dict[str, str] = {}

        progress_bar = st.progress(0)
        status_text  = st.empty()
        log_area     = st.container()

        total = len(all_urls)

        for idx, url in enumerate(all_urls):
            role = "Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹" if url == target_url else f"ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚ {idx + 1}/{len(competitor_urls)}"
            netloc = urlparse(url).netloc
            progress_bar.progress(idx / total, text=f"â³ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ {role}: {netloc}")
            status_text.info(f"Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ: **{url}**")

            html, err = fetch_via_scrapingbee(url, api_key, timeout)

            if err:
                errors_log[url] = err
                all_results[url] = []
                log_area.warning(f"âš ï¸ {netloc}: {err}")
            else:
                blocks = extract_blocks(html, mode_key)
                all_results[url] = blocks
                log_area.success(f"âœ… {netloc} â€” Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ²: **{len(blocks)}**")

        progress_bar.progress(1.0, text="ğŸ“Š Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑ Excel...")
        status_text.info("Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ Excel-Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚...")

        try:
            excel_bytes = make_excel(target_url, competitor_urls, all_results)
            st.session_state["excel_bytes"] = excel_bytes
            st.session_state["excel_ready"] = True
        except Exception as ex:
            st.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Excel: {ex}")
            st.session_state["excel_ready"] = False

        progress_bar.progress(1.0, text="âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾!")
        status_text.success("ğŸ‰ ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½!")

        # â”€â”€ ĞŸÑ€ĞµĞ²ÑŒÑ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.divider()
        st.subheader("ğŸ“‹ ĞšÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹")

        cols = st.columns(min(len(all_urls), 6))
        for i, url in enumerate(all_urls):
            blocks = all_results.get(url, [])
            netloc = urlparse(url).netloc
            label  = f"â˜… {netloc}" if url == target_url else netloc
            with cols[i % len(cols)]:
                if url in errors_log:
                    st.metric(label, "ĞÑˆĞ¸Ğ±ĞºĞ°", delta="âš ï¸", delta_color="off")
                else:
                    groups_found = len(set(b["group"] for b in blocks))
                    st.metric(label, f"{len(blocks)} Ğ±Ğ»Ğ¾ĞºĞ¾Ğ²", f"{groups_found} Ğ³Ñ€ÑƒĞ¿Ğ¿")

        # Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ²
        target_blocks = all_results.get(target_url, [])
        target_groups = set(b["group"] for b in target_blocks)
        missing = []
        for cu in competitor_urls:
            for b in all_results.get(cu, []):
                if b["group"] not in target_groups:
                    missing.append(b["group"])

        from collections import Counter
        missing_freq = Counter(missing)
        if missing_freq:
            st.divider()
            st.subheader("ğŸ”´ Ğ‘Ğ»Ğ¾ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ½ĞµÑ‚ Ğ½Ğ° Ğ²Ğ°ÑˆĞµĞ¼ ÑĞ°Ğ¹Ñ‚Ğµ")
            miss_df = pd.DataFrame(
                [(g, f, "ğŸ”´ ĞĞ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾" if f >= 3 else "ğŸŸ¡ Ğ–ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾" if f == 2 else "âšª ĞŸĞ¾ Ğ¶ĞµĞ»Ğ°Ğ½Ğ¸Ñ")
                 for g, f in missing_freq.most_common()],
                columns=["Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ° Ğ±Ğ»Ğ¾ĞºĞ°", "Ğ§Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° Ñƒ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ²", "Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ñ"]
            )
            st.dataframe(miss_df, use_container_width=True, hide_index=True)

    # â”€â”€ ĞšĞ½Ğ¾Ğ¿ĞºĞ° ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if st.session_state.get("excel_ready"):
        with col_dl:
            st.download_button(
                label="ğŸ“¥ Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Excel",
                data=st.session_state["excel_bytes"],
                file_name="ĞºĞ½Ğ´Ñ€_Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True,
            )


if __name__ == "__main__":
    main()
